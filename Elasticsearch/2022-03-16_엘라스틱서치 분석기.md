### 1.엘라스틱서치 분석기

분석기는 text 타입의 필드에 사용되어 데이터를 분석한다. 텍스트가 분석되면 개별 텀으로 나뉘어 형태소 형태로 분석된다. 해당 형태소는 특정 원칙에 의해 필터링되어 단어가 삭제되거나 추가,수정되고 최종적으로 역색인된다.

엘라스틱서치는 각각 다른 언어의 형태소를 분석할 수 있도록 언어별로 분석기를 제공한다. 

엘라스틱서치에서 제공하는 Analyzer API를 이용해 손쉽게 분석 결과를 확인할 수 있다.

```bash
POST _analyze
{
	"analyzer" : "standard",
	"text" : "우리나라가 좋은나라, 대한민국 화이팅"
}
```

분석기는 대상 문장을 token 값으로 표시하게 된다. 예시의 분석 결과는 주어진 문장을 ["우리나가","좋은나라","대한민국","화이팅] 으로 총 4부분의 token으로 분리한다. 예시에서는 standard analyzer를 사용했기 때문에 형태소 분석은 하지 않았다.

#### 1.1 분석기의 구조

분석기는 기본적으로 다음과 같은 프로세스로 동작한다.

1. 문장을 특정한 규칙에 의해 수정한다.
2. 수정한 문장을 개별 토큰으로 분리한다.
3. 개별 토큰을 특정한 규칙에 의해 변경한다.

이 세가지 동작은 특성에 의해 각각 다음과 같은 용어로 불린다.

- CHARACTER FILTER

문장을 분석하기 전에 입력 텍스트에 대해 특정한 단어를 변경하거나 HTML과 같은 태그를 제거하는 역할을 하는 필터다.

- TOKENIZER FILTER

분석기를 구성할 때 하나만 사용할 수 있으며 텍스트를 어떻게 나눌 것인지 정의한다.

- TOKEN FILTER

토큰화된 단어를 하나씩 필터링해서 사용자가 원하는 토큰으로 변환한다.

##### 1.1.1 분석기 사용법

엘라스틱서치는 루씬에 존재하는 기본 분석기를 별도의 정의 없이 사용할 수 있게 미리 정의해서 제공한다. 이러한 분석기를 사용하기 위해 엘라스틱 서치에서는 _analze API를 제공한다.

##### 분석기를 이용한 분석

```bash
POST _analyze
{
	"analyzer" : "standard",
	"text" : "캐리비안의 해적"
}
```

##### 필드를 이용한 분석

인덱스를 설정할 때 분석기를 직접 설정할 수 있다. 

```

```

##### 색인과 검색 시 분석기를 각각 설정

분석기는 색인할 때 사용되는 index analyzer와 검색할 때 사용되는 search analyzer로 구분해서 구성할 수 있다. 인덱스를 생성할 때 색인용과 검색용 분석기를 각각 정으하고 적용하고자 하는 필드에 원하는 분석기를 지정하면 된다.

```bash
PUT movie_analyzer
{
	"settings" : {
		"number_of_shards" : 5,
		"number_of_replicas" : 1
	},
	"analysis" : {
		"analyzer" : {
			"movie_lower_analyzer" : {
				"type" : "custom",
				"tokenizer" : "standard",
				"filter" : [
				"lowercase"
				]
			},
			"movie_stop_analyer" : {
				"type" : "custom",
				"tokenizer" : "standard",
				"filter" : [
				"lowercase",
				"english_stop"
				]
			}
		},
		"filter" : {
			"english_stop" : {
				"type" : "stop",
				"stopwords" : "_english"
				}
			}
		}
	},
	"mappings" : {
		"properties" : {
			"title" : {
				"type" : "text",
				"analyzer" : "movie_stop_analyzer",
				"search_analyzer" : "movie_lower_analyzer"
				}
			}
		}
	}
}
```

##### 1.1.2 대표적인 분석기

##### standard analyzer

인덱스를 생성할 때 settings에 analyzer를 정의하게 된다. 하지만 아무런 정의를 하지 않고 필드의 데이터 타입을 text로 설정한다면 기본적으로 standard analyzer를 사용하게 된다. 이 분석기는 공백 혹은 특수 기호를 기준으로 토큰을 분리하고 모든 문자를 소문자로 변경하는 토큰 필터를 사용한다.

```bash
POST movie_analyzer/_search
{
	"analyzer" : "standard",
	"text" : "Harry Potter and the Chamber of Secrets"
}
```

##### whitespace analyzer

공백 문자열을 기준으로 토큰을 분리하는 간단한 분석기다. standard와 다르게 문자를 소문자로 변경시키지 않는다.

```bash
POST movie_analyzer/_search
{
	"analyzer" : "whitespace",
	"text" : "~~"
}
```

##### keyword analyzer

전체 입력 문자열을 하나의 키워드처럼 처리한다. 토큰화 작업을 하지 않는다.

```bash
POST movie_analyzer/_search
{
	"analyzer" : "keyword",
	"text" : "~~"
}
```

#### 1.2 전처리 필터

토크나이저 내부에서 전처리가 가능하기 때문에 전처리 필터는 활용도가 많이 떨어진다. 

##### HTML strip char 필터

문장에서 HTML을 제거하는 전처리 필터다.

| 파라미터     | 설명                                                        |
| ------------ | ----------------------------------------------------------- |
| escaped_tags | 특정 태그만 삭제한다. 기본값으로 HTML 태그를 전부 삭제한다. |

```bash
PUT movie_html_analyzer
{
	"settings" : {
		"analysis" : {
			"analyzer" : {
				"html_strip_analyzer" : {
					"tokenizer" : "keyword",
					"char_filter" : [
						"html_stop_filter"
					]
				}
			},
			"char_filter" : {
				"html_stop_filter" : {
					"type" : "html_strip",
					"escaped_tags" : [
						"b"
					]
				}
			}
		}
	}
}
```

#### 1.3 토크나이저 필터

토크나이저 필터는 분석기의 핵심 요소다. char filter를 거쳐 토크나이저 필터로 문서가 넘어오면 해당 텍스트는 tokenizer의 특성에 맞게 적절히 분해된다. 어떤 토크나이저를 사용하느냐에 따라 분석기의 전체적인 성격이 결정된다.

##### standard 토크나이저

일반적으로 사용하는 토크나이저로서 대부분의 기호를 만나면 토큰으로 나눈다.

##### whitespace 토크나이저

공백을 만나면 텍스트를 토큰화한다.

##### Ngram 토크나이저

Ngram은 기본적으로 한 글자씩 토큰화 한다. Ngram에 특정 문자를 지정할 수도 있으며, 이 경우 지정된 문자의 목록 중 하나를 만날 때 마다 단어를 자른다.

| 파라미터    | 설명                                        |
| ----------- | ------------------------------------------- |
| min_gram    | Ngram을 적용할 문자의 최소 길이를 나타낸다. |
| max_gram    | Ngram을 적용할 문자의 최대 길이를 나타낸다. |
| token_chars | 토큰에 포함할 문자열을 지정한다.            |

```bash
PUT movie_ngram_analzer
{
	"settings" : {
		"analysis" : {
			"analyzer" : {
				"ngram_analyzer" : {
					"tokenizer" : "ngram_tokenizer"
				}
			},
			"tokenizer" : {
				"ngram_tokenizer" : {
					"type" : "ngram",
					"min_gram" : 3,
					"max_gram" : 3,
					"token_char" : "letter"
				}
			}
		}
	}
}
```

위의 분석기를 이용해 텍스트를 분리한다면 3글자 묶음을 글자 한칸씩 전진시키면서 보여준다.

즉 harry potter 를 분석하게 된다면 ['har','arr','rry','pot','ott','ttr','ter'] 이러한 토큰을 생성하게 된다.   

##### Edge Ngram 토크나이저

지정된 문자의 목록 중 하나를 만날 때마다 시작 부분을 고정시켜 단어를 자르는 방식으로 사용하는 토크나이저다.

| 파라미터    | 설명                                        |
| ----------- | ------------------------------------------- |
| min_gram    | Ngram을 적용할 문자의 최소 길이를 나타낸다. |
| max_gram    | Ngram을 적용할 문자의 최대 길이를 나타낸다. |
| token_chars | 토큰에 포함할 문자열을 지정한다.            |

```bash
PUT movie_ngram_analzer
{
	"settings" : {
		"analysis" : {
			"analyzer" : {
				"edge_analyzer" : {
					"tokenizer" : "edge_ngram_tokenizer"
				}
			},
			"tokenizer" : {
				"edge_ngram_tokenizer" : {
					"type" : "edge_ngram",
					"min_gram" : 2,
					"max_gram" : 10,
					"token_char" : "letter"
				}
			}
		}
	}
}
```

##### keyword 토크나이저

keyword 토크나이저는 텍스트를 하나의 토큰으로 만든다.

#### 1.4 토큰 필터

토큰 필터는 토크나이저에서 분리된 토큰들을 변형하거나 추가, 삭제할 때 사용하는 필터다. 여기서는 가장 간단한 형태의 standard analyzer를 이용해 토큰을 분리하고 토큰 필터의 특성을 알아볼 것이다.

##### ascii folding 토큰 필터

아스키 코드에 해당하는 127개의 알파벳,숫자,기호에 해당하지 않는 경우 문자를 ASCII 요소로 변경한다. 

```bash
PUT movie_af_analyzer
{
  "settings": {
    "analysis": {
      "analyzer": {
        "af_analyzer" : {
          "tokenizer" : "standard",
          "filter" : [
            "asciifolding"
            ]
        }
      }
    }
  }
}
```

##### lowercase,uppercase 토큰 필터

각각 토큰을 소문자,대문자로 변환시켜주는 기능을 한다.

```bash
PUT movie_lowercase_analyzer
{
	"settings" : {
		"analysis" : {
			"analyzer" : {
				"lowercase_analyzer" : {
					"tokenizer" : "stardard",
					"filter" : [
						"lowercase"
					]
				}
			}
		}
	}
}
#Uppercase역시 똑같이 설정해주면 된다.
```

 ##### stop 토큰 필터

불용어로 등록할 사전을 구축해서 사용하는 필터를 의미한다. 인덱스로 만들고 싶지 않거나 검색되지 않게 하고 싶은 단어를 등록해서 해당 단어에 대한 불용어 사전을 구축한다.

| 파라미터       | 설명                                                         |
| -------------- | ------------------------------------------------------------ |
| stopwords      | 불용어를 매핑에 직접 등록해서 사용한다.                      |
| stopwords_path | 불용어 사전이 존재하는 경로를 지정한다. 해당 경로는 엘라스틱서치 서버가 있는 config 폴더 안에 생성한다. |
| ignore_case    | true로 지정할 경우 모든 단어를 소문자로 변경해서 저장한다. 기본값은 false |

##### synonym 토큰 필터

동의어를 처리할 수 있는 필터다. 

| 파라미터      | 설명                                                         |
| ------------- | ------------------------------------------------------------ |
| synonyms      | 동의어로 사용할 단어를 등록한다.                             |
| sysnoyms_path | 파일로 관리할 경우 엘라스틱서치 서버의 config 폴더 아래에 생성한다. |

```bash
PUT movie_syno_analyzer
{
  "settings": {
    "analysis": {
      "analyzer": {
        "syno_analyzer" : {
          "tokenizer" : "standard",
          "filter" : [
            "synonym_filter"
            ]
        }
      },
      "filter": {
        "synonym_filter" : {
          "type" : "synonym",
          "synonyms" : [
            "Harry => 해리"
            ]
        
        }
      }
    }
  }
}
```

##### trim 토큰 필터

앞뒤 공백을 제거하는 토큰 필터다.

```bash
PUT movie_trim_analyzer
{
	"settings" : {
		"analysis" : {
			"analyzer" : {
				"trim_analyzer" : {
					"tokenizer" : "standard",
					"filter" : [
						"trim",
						"lowercase"
					]
				}
			}
		}
	}
}
```

#### 1.5 동의어 사전 만들기

동의어 파일은 config 디렉터리에 생성해야 한다.

```bash
PUT movie_syno_analyzer
{
  "settings": {
    "analysis": {
      "analyzer": {
        "syno_analyzer" : {
          "tokenizer" : "standard",
          "filter" : [
            "synonym_filter"
            ]
        }
      },
      "filter": {
        "synonym_filter" : {
          "type" : "synonym",
          "synonyms_path" : "analysis/synonym.txt" 
          }        
        }
      }
    }
  }
}
```

동의어 파일 안의 내용은 다음과 같다.

```
Elasticsearch, 엘라스틱서치
Harry => 해리
```

위의 경우는 앞의 토큰과 일치하는 동의어인 엘라스틱서치를 새로운 토큰으로 추가한다. 아래의 경우는 앞의 단어와 일치하는 토큰을 뒤의 단어로 치환한다.

동의어 사전의 내용을 바꿀 경우 인덱스를 reload해야한다.

먼저 인덱스를 Close한다.

```
POST movie_analyzer/_close
```

이후 동의어 사전을 저장한 후 인덱스를 Open한다.

```
POST movie_analyzer/_open
```

