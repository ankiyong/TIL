## 엘라스틱서치 운영 클러스터 구축

엘라스틱 서치의 운영 클러스터를 구축할때 최소 3개의 master node가 필요하다. 1개가 다운됐을 때 split brain없이 서비스의 지속을 유지하기 위한 최소 기준이다. data node는 2대 이상이 필요하다. 하나의 data node를 replica로 활용하여 장애 발생에 대응이 가능해진다.

### 1.클러스터 구성

3개의 노드로 클러스터를 구성하는 실습을 진행할 것이다.

#### 1.1 노드 설치

아래 그림과 같이 elasticsearch 폴더를 3개 만들어 각각 node1,node2,node3으로 이름을 바꿔준다. 각 폴더의 elasticsearch는 하나의 노드로서 동작하게 될 것이다. kibana역시 압축을 풀어서 동일한 폴더 내에 위치 시킨다.

![node](C:\Users\pop24\Desktop\source_code\image\nodes.png)

이후 각각의 node/config 폴더에 elasticsearch.yml 파일을 수정해준다.

```
cluster.name: my-cluster
node.name: node1
network.host: [_local_]
http.port: 9200
transport.port: 9300
discovery.seed_hosts: ["localhost:9300","localhost:9310","localhost:9320"]
cluster.initial_master_nodes: ["node1","node2","node3"]
```

#### 1.2 운영 모드 설정



```
## JVM configuration

################################################################
## IMPORTANT: JVM heap size
################################################################
##
## You should always set the min and max JVM heap
## size to the same value. For example, to set
## the heap to 4 GB, set:
##
## -Xms4g
## -Xmx4g
##
## See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html
## for more information
##
################################################################

-Xms1g
-Xmx1g

1.위의 안내 사항대로 Xms(최소 힙 크기)와 Xmx(최대 힙 크기)를 동일하게 할당해야 한다.
2.힙 크기는 최대 물리 메모리의 절반으로 한다.
3.힙 크기는 최대 30~31GB 수준을 넘기지 않는다.



################################################################
## Expert settings
################################################################
##
## All settings below this section are considered
## expert settings. Don't tamper with them unless
## you understand what you are doing
##
################################################################

## GC configuration
8-13:-XX:+UseConcMarkSweepGC
8-13:-XX:CMSInitiatingOccupancyFraction=75
8-13:-XX:+UseCMSInitiatingOccupancyOnly

## G1GC Configuration
# NOTE: G1 GC is only supported on JDK version 10 or later
# to use G1GC, uncomment the next two lines and update the version on the
# following three lines to your version of the JDK
# 10-13:-XX:-UseConcMarkSweepGC
# 10-13:-XX:-UseCMSInitiatingOccupancyOnly
14-:-XX:+UseG1GC
14-:-XX:G1ReservePercent=25
14-:-XX:InitiatingHeapOccupancyPercent=30

## JVM temporary directory
-Djava.io.tmpdir=${ES_TMPDIR}

## heap dumps

# generate a heap dump when an allocation from the Java heap fails
# heap dumps are created in the working directory of the JVM
-XX:+HeapDumpOnOutOfMemoryError

# specify an alternative path for heap dumps; ensure the directory exists and
# has sufficient space
-XX:HeapDumpPath=data

# specify an alternative path for JVM fatal error logs
-XX:ErrorFile=logs/hs_err_pid%p.log

## JDK 8 GC logging
8:-XX:+PrintGCDetails
8:-XX:+PrintGCDateStamps
8:-XX:+PrintTenuringDistribution
8:-XX:+PrintGCApplicationStoppedTime
8:-Xloggc:logs/gc.log
8:-XX:+UseGCLogFileRotation
8:-XX:NumberOfGCLogFiles=32
8:-XX:GCLogFileSize=64m

# JDK 9+ GC logging
9-:-Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m

```

<hr/>

메모리 스왑 기능을 비활성화하기 위해 각 node의 elasticsearch.yml파일을 수정해야 한다.

```
cluster.name: my-cluster
node.name: node1
network.host: [_local_]
http.port: 9200
transport.port: 9300
discovery.seed_hosts: ["localhost:9300","localhost:9310","localhost:9320"]
cluster.initial_master_nodes: ["node1","node2","node3"]

bootstrap.memory_lock: true 라인을 추가해준다.

```

bootstrap.memory_lock 설정은 노드 실행 시 시스템의 물리 메모리를 미리 할당받아 스왑 영역을 사용하지 않도록 방어하는 설정이다. 메모리 부족으로 인해 디스크의 스왑 영역을 참조할 경우 심각한 성능 저하가 발생할 수 있기 떄문에 설정이 필수적이다.

<hr/>

다음으로 리눅스 시스템 수준에서 거려 있는 제한들을 해제해줄 필요가 있다. 

```
cent - nofile 65535
cent - nproc 4096
cent soft memlock unlimited
cent hard memlock unlimited
```

nofile는 최대 파일 디스크립터 수를 의미하며 시스템에서 최대로 열 수 있는 파일 수를 제한한다.

##### *es는 샤드 하나당 몇 개씩의 파일들을 열어놓고 사용하며, 인덱스가 늘어남에 비례하여 이 수가 증가하기 때문에 운영 초기에는 문제가 없다가도 시간이 지남에 따라 샤드가 늘어나 파일 디스크립터 수 제한에 의해 문제가 발생하는 경우가 있어서 넉넉하게 설정해준다.*

nproc는 최대 프로세스 수를 제한하는 설정이다.

##### *검색,인덱싱 등 다양한 작업을 실행하다 보면 프로세스가 많이 생성되기 때문에 넉넉하게 잡아주는 편이 좋다.*

memlock는 메모리 내 주소 공간의 최대 크기를 지정하는데, 엘라스틱 가이드에 따라 무한댈고 잡아준다.

reboot한 후 ulimit 명령어로 확인해 보면 내용이 바뀌어 있다.

확인해보면 openfile은 설정해준 값이 적용 안된 것을 확인할 수 있다. 다음 2개 파일을 수정해 준다.

```
sudo vi /etc/systemd/system.conf
sudo vi /etc/sysstemd/user.conf
```

![nofile](C:\Users\pop24\Desktop\source_code\image\nofile_setting.png)

수정한 후 다시 reboot해보면 수정된 것을 확인할 수 있다.

<hr/>

다음으로는 최대 가상 메모리 수를 설정해 줘야 한다.

##### *es는 기본적으로 파일 입출력 성능 향상을 위해 파일을 메모리에 매핑하는 mmap 파일 시스템을 사용하는데, 이는 가상 메모리 공간을 사용하므로 충분한 공간을 확보할 필요가 있다.*

/etc/sysctl.conf 파일을 수정해 준다.

```
vm.max_map_count = 262144
```

<hr/>

#### 1.3 실행과 구성 확인

각 노드의 구성을 마쳤다면 모든 노드를 실행시키고 다음 명령어릁 통해 정상적으로 작동하는지 확인할 수 있다.

```
curl -XGET http://localhost:9200/_cluster/health?pretty
```

다음과 같은 결과가 나오면 정상적으로 실행되는 것이다. 

![node](C:\Users\pop24\Desktop\source_code\image\node_run.png)

마지막으로 kibana에서 3개의 노드에 접근할 수 있도록 설정해야 한다. kibana.yml 파일을 수정해 준다.

```
server.host: 0.0.0.0
elasticsearch.hosts: ['http://loclahost:9200','http://localhost:9210','http://localhost:9220']
```

