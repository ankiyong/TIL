상가정보를 가지고 검색 기능 구현

필요없는 컬럼이 너무 많아서 검색과 결과에 사용하지 않을 컬럼은 모두 삭제

편의를 위해 컬럼 이름을 영어로 조정함

null 값이 많이 있어서 이것들 조정해야함 -> es에서 매핑으로 해결 가능

default_value 값을 설정해서 해결 시도

```json
floor : {
	"type" : "integer",
	"null_value" : "1"
}
```

이렇게 설정하면 1로 검색했을 때 층 정보가 null인 data가 검색되지만 원본 데이터가 변경되는 것은 아니기 때문에 결과 정보로 사용하기에 부적합

그래서 원본 데이터를 pandas로 수정



주소,상권분류명,지점명,상호명을 검색 조건으로 하여 상호명,지점명,주소,층을 결과로 보여주려고 함

주소는 text type으로 매핑하여 구,동 이름으로도 검색이 가능하게 할 것
지점명,상호명은 keyword로 매핑하여 검색이 가능하도록 할 것 ex) 굽네치킨 구로점



    lot_addr" : "서울특별시 / 종로구 / 봉익동 / 43-1",
    봉익동을 검색하면 봉익동의 모든 상권 정보가 나오게 매핑을 계획
    하지만 기본 nori_analyzer는 위의 주소를 서울/특별/시/종로/구/봉익/동/43/1 로 토큰화 한다.
    서울시/ 종로구/ 봉익동 혹은
    서울시 종로구 봉익동 43-1로 검색되게 만들 것
    standard analyzer 적용하면 서울특별시/종로구/봉익동/43/1 로 tokenize 한다
    그리고 서울시 라는 토큰이 생성되지 않아 원하는 검색어로 검색되지 않기 때문에 동의어 사전을 이용하여 서울특별시와 서울시를 동의어로 만들어 준다. 


서울시 구로동 검색했을 때 서울/시/구로/동 으로 토큰화 하기 때문에 검색이 잘 되지 않는 문제 발생함

그래서 search analyzer를 따로 지정해야함

search 할 때 indexing에서 시,구,동 빼고 진행하면 token이 [서울,구로]만 남게 될 것이다.

그러면 index analyzer가 서울,구로만 가지고 있으면 검색이 될 것이다. 

```
동의어 사전 없이 stop_filter로 해결
stop_filter list에 특별,구,동,시 를 명시해두면 
서울시 구로동 / 서우륵별시 구로구 구로동 / 서울시 구로구 구로동
서울  / 구로       서울 / 구로 / 구로     서울 / 구로 / 구로
이렇게 토큰화 되기 때문에 원하는 모양의 토큰 결과를 확인하게 됨
```

토큰은 원하는 모양으로 만들었는데 검색이 문제가 됨

서울시 구로동을 검색하면 서울 or 구로 의 검색 결과를 보여주기 때문에 두 단어를 and 로 묶어서 검색하는 기능이 필요.

만약 안된다면 주소를 term으로 나눠서 keyword검색을 해야할 것 같다. x

```
match {
	"query" : {
		"field" : "text",
		"operator" : "and"
	}
}
위의 쿼리를 적용해서 검색할 때 두 단어를 and로 묶어서 검색하는 것에 성공했다.
```

여전히 개선할 점이 있다. 

구로구 구로동과 같이 구,동의 이름이 같은 경우 토큰이 동일하기 떄문에 검색이 원활하게 되지 않는다. 

예를들어 구로구 구로동 을 검색하면 구로구 오류동의 데이터가 나오게 된다. "구로"라는 토큰과 일치하는 값을 찾기 때문이다. 이걸 어떻게 해결할지 고민해야한다. 

```
주소 확인사항
주소로 검색했을 때 검색이 가능한가?
서울시 신사동 으로 검색했을 떄 검색이 가능한가 ?
서울시 강남구 신사동으로 검색했을 떄 검색이 가능한가 ?
강남구 신사동 으로 검색했을 떄 검색이 가능한가 ?
신사동 으로 검색했을 떄 검색이 가능한가 ?
```

<hr/>

모든 검색 값에 대해서 default 값이 필요하다. 지점 같은 경우는 임의로 기본값을 본점으로 잡아뒀지만 search를 실행하게 되면 값을 비워두고 검색할 수 없기 때문에 기본값 설정이 있다면 기본값을 설정해야 할 것이다. 없다면 어떻게 처리할지 고민해봐야함

```
wildcard query사용해서 해결
"wildcard" : {
	"stie_nm" : "{{query_string}}"
}
이렇게 하면 기본값으로 *을 둬서 모든 값이 검색 가능한 상태를 만들 수 있다.
```

<hr/>

```
현재까지 searchtemplate와 index 상태
#index
PUT text_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "nori_analyzer" : {
          "type" : "custom",
          "tokenizer" : "nori_tokenizer",
          "filter" : "synonym"
        },
        "stop_nori" : {
          "type" : "custom",
          "tokenizer" : "nori_tokenizer",
          "filter" : "stop_filter"
        },
        "edge_ngram" : {
          "tokenizer" : "edge_ngram_tokenizer",
          "filter" : "synonym"
        },
        "ngram" : {
          "tokenizer" : "ngram_tokenizer",
          "filter" : "synonym"
        }
      },
      "tokenizer": {
        "edge_ngram_tokenizer": {
          "type" : "edge_ngram",
          "min_gram" : 2,
          "max_gram" : 20,
          "token_char" : "letter"
        },
        "ngram_tokenizer" : {
          "type" : "ngram",
          "min_gram" : 2,
          "max_gram" : 3,
          "token_char" : "letter"
        }
      }, 
      "filter" : {
        "synonym" : {
          "type" : "synonym",
          "synonyms_path" : "analysis/synonym.txt"
        },
        "stop_filter" : {
          "type" : "stop",
          "stopwords_path" : "analysis/stop_dic.txt"
        }
      }
    }
  }, 
  "mappings": {
    "properties": {
      "name" : {
        "type" : "text"
      },
      "site_nm" : {
        "type" : "keyword"
      },
      "middle_cate" : {
        "type" : "text",
        "analyzer": "stop_nori",
        "search_analyzer": "edge_ngram"
      },
      "detail_cate" : {
        "type" : "text",
        "analyzer": "stop_nori",
        "search_analyzer": "edge_ngram"
      },
      "cate" : {
        "type" : "text",
        "analyzer": "stop_nori",
        "search_analyzer": "edge_ngram"
      },
      "addr" : {
        "type" : "text",
        "analyzer": "nori_analyzer",
        "search_analyzer": "stop_nori"
      },
      "floor" : {
        "type" : "keyword"
      }
    }
  }
}
===========================================================
#search template
PUT _scripts/test_template
{
  "script" : {
    "lang": "mustache",
    "source": {
      
      "query" : {
        "bool" : {
          "must" : [
            {
              "wildcard" : {
              "site_nm" : "{{query_string}}"
              }
            },
            {
              "wildcard" : {
              "name" : "{{name_string}}"
              }
            },
            {
              "match" : {
                "addr" : {
                  "query" : "{{addr_string}}",
                  "operator" : "and"
                }
              }
            },
            {
              "query_string" :  {
                "query" : "{{cate_string}}",
                "fields" : ["detail_cate","middle_cate","cate"]
              }
            }
          ]
        }
      },
      "size" : "{{size}}"
    }
  }
}
```

<hr/>



    "detail_cate" : "시계/귀금속"
    "cate" : "시계 및 귀금속 소매업"
    "middle_cate" : "시계/귀금속소매"
    분류명들은 multi_match를 사용해서 한꺼번에 검색이 가능하도록 구성
    detail_cate의 경우 /,- 과 같은 특수문자 존재
    cate는 소매업
    middle는 소매
    
    /,-삭제 및 소매업 소매 중개업

검색할 때 영어 검색이 안되는 부분이 있어서 확인해봄

-> search analyzer는 standard여서 소문자로 바꿔줬지만 index analyzer는 ngram으로 소문자화 하지 않았기 땓문에 서로 토큰이 달라서 검색이 안됐음. 그래서 ngrma에 lowercase filter를 추가해줬음

<hr/>

날짜형식 추가해서 작업

date와 timestamp를 추가해서 작업하기로 함 

range query를 사용해보기 위함 

```json
      "date" : {
        "type" : "date",
        "format" : "yyyy/MM/dd||epoch_millis"
      },
      "timestamp" : {
        "type" : "date",
        "format" : "yyyy-MM-dd HH:mm:ss||epoch_millis"
      }
```







```
지금까지 완료된 것들

POST text_index/_search/template
{
  "id" : "test_template",
  "params" : {
    "floor_level" : "",
    "address" : "서울",
    "address_operator" : "and",
    "category" : "건강",
    "category_operator" : "and",
    "site_name" : "*",
    "store_name" : "*",
    "gte_date" : "2020/09/13",
    "lte_date" : "2020/09/13",
    "time" : "2022-03-30 01:01",
    "size" : 10000
  }
}


PUT _scripts/test_template
{
  "script" : {
    "lang": "mustache",
    "source": {
      
      "query" : {
        "bool" : {
          "must_not" : [
            {
              "wildcard" : {
                "floor" : "{{floor_level}}"
              }
            }
          ],
          "must" : [
            {
              "wildcard" : {
              "site_nm" : "{{site_name}}"
              }
            },
            {
              "wildcard" : {
              "name" : "{{store_name}}"
              }
            },
            {
              "match" : {
                "addr" : {
                  "query" : "{{address}}",
                  "operator" : "{{address_operator}}"
                }
              }
            },
            {
              "multi_match" :  {
                "query" : "{{category}}",
                "fields" : ["detail_cate","middle_cate","cate"],
                "operator" : "{{category_operator}}"
              }
            },
          {
            "range" : {
              "date" : {
                "gte" : "{{gte_date}}",
                "lte" : "{{lte_date}}",
                "format" : "yyyy/MM/dd"
                }
              }
            },
          {
            "range" : {
              "@timestamp" : {
                "gte" : "{{time}}",
                "format" : "yyyy-MM-dd HH:mm||date_optional_time"
                }
              }
            }
          ]
        }
      },
      "size" : "{{size}}"
    }
  }
}


GET text_index/_search
DELETE text_index
PUT text_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "nori_analyzer" : {
          "type" : "custom",
          "tokenizer" : "nori_tokenizer",
          "filter" : "synonym"
        },
        "stop_nori" : {
          "type" : "custom",
          "tokenizer" : "nori_tokenizer",
          "filter" : "stop_filter"
        },
        "ngram" : {
          "tokenizer" : "ngram_tokenizer",
          "filter" : [
            "lowercase",
            "synonym",
            "stop_filter"
            ]
        }
      },
      "tokenizer": {
        "ngram_tokenizer" : {
          "type" : "ngram",
          "min_gram" : 2,
          "max_gram" : 3,
          "token_char" : "letter"
        }
      }, 
      "filter" : {
        "synonym" : {
          "type" : "synonym",
          "synonyms_path" : "analysis/synonym.txt"
        },
        "stop_filter" : {
          "type" : "stop",
          "stopwords_path" : "analysis/stop_dic.txt"
        }
      }
    }
  }, 
  "mappings": {
    "properties": {
      "name" : {
        "type" : "text"
      },
      "site_nm" : {
        "type" : "keyword"
      },
      "middle_cate" : {
        "type" : "text",
        "analyzer": "ngram",
        "search_analyzer": "standard"
      },
      "detail_cate" : {
        "type" : "text",
        "analyzer": "ngram",
        "search_analyzer": "standard"
      },
      "cate" : {
        "type" : "text",
        "analyzer": "ngram",
        "search_analyzer": "standard"
      },
      "addr" : {
        "type" : "text",
        "analyzer": "nori_analyzer",
        "search_analyzer": "stop_nori"
      },
      "load_addr" : {
        "type" : "text",
        "analyzer": "nori_analyzer"
      },
      "state" : {
        "type" : "text",
        "fields": {
          "keyword" : {
            "type" : "keyword"
          }
        }
      },
      "city" : {
        "type" : "text",
        "fields": {
          "keyword" : {
            "type" : "keyword"
          }
        }
      },
      "dong" : {
        "type" : "text",
        "fields": {
          "keyword" : {
            "type" : "keyword"
          }
        }
      },
      "load" : {
        "type" : "keyword"
      },
      "floor" : {
        "type" : "keyword"
      },
      "date" : {
        "type" : "date",
        "format" : "yyyy/MM/dd||epoch_millis"
      },
      "@timestamp" : {
        "type" : "date",
        "format" : "yyyy-MM-dd HH:mm:ss.S||date_optional_time"
      }
    }
  }
}


#이건 실험 결과
GET time/_search
{
  "query": {
    "range" : {
      "time": {
        "gte" : "2021-03-30 02:01",
        "format" : "yyyy-MM-dd HH:mm||date_optional_time"
      }
    }
  }
}

GET time/_search

PUT time/_doc/2
{
  "time" : "2022-03-30T02:01:35.656Z"
}

PUT time/_doc/2
{
  "time" : "20220330020135656"
}

DELETE time

PUT time
{
  "mappings": {
    "properties": {
      "time" : {
        "type" : "date",
        "format" : "yyyy-MM-dd HH:mm:ss.SSS||date_optional_time"
      }
    }
  }
}

```

<hr/>

기존에 도로명을 이용할 일이 있을 것 같아서 도로명 주소를 ls에서 나눠서 넣어 뒀었다 .

```json
input { 
	file {
		path => "C:/logstash-7.10.1/config/seoul_data/seoul4.csv"
		start_position => "beginning"
		sincedb_path => "nul"
	}
}
filter {
	csv{
		separator => ","
		skip_header => true		
		columns => ["name","site_nm","middle_cate","detail_cate","cate","addr","load_addr","floor","date"]
	}
	dissect{
		mapping => {"addr" => "%{state} %{city} %{dong} %{post}"}
		mapping => {"load_addr" => "%{state_l} %{city_l} %{load} %{address}"}
	}
	mutate{
		remove_field => ["message","@version","host","path","state_l","city_l","post","address"]
	}

}


output {
	stdout{}
}
```

그런데 주소 검색에서 도로명 주소는 쓰지 않게 되어 ls conf 파일으 수정해서 해당 부분을 제거했다.

```
input { 
	file {
		path => "C:/logstash-7.10.1/config/seoul_data/seoul4.csv"
		start_position => "beginning"
		sincedb_path => "nul"
	}
}
filter {
	csv{
		separator => ","
		skip_header => true		
		columns => ["name","site_nm","middle_cate","detail_cate","cate","addr","load_addr","floor","date"]
	}
	dissect{
		mapping => {"addr" => "%{state} %{city} %{dong} %{post}"}
	}
	mutate{
		remove_field => ["message","@version","host","path","post"]
	}

}


output {
	stdout{}
}
```

<hr/>

alias 기능을 사용해 보기 위해 기존의 서울 데이터와 동일한 형태의 인천 데이터를 준비했다.

전처리 과정을 동일하니 생략하도록 하겠다. 

그리고  seoul_store 와 형식이 동일한 incheon_store 을 만들었다. 각각 서울, 인천의 데이터를 인덱싱 할 것이다. 

데이터가 색인되면 alias 설정을 할 수 있다. 

```json
POST _aliases
{
	"actions" : [
		{
            "add" : {
                "index" : "seoul_store",
                "alias" : "seoul"
            }
         },
         {
         	"add" : {
         		"index" : "incheon_store",
				"alias" : "incheon"
        	}
        }
	]
}
```

이제 각각 seoul,incheon 이라는 별칭으로 index의 조작이 가능해졌다. 

각각의 인덱스를 하나의 별칭으로 묶어서 사용할 수도 있다. 

```json
POST _aliases
{
	"actions" : [
		{
            "add" : {
                "index" : "seoul_store",
                "alias" : "store_info"
            }
         },
         {
         	"add" : {
         		"index" : "incheon_store",
				"alias" : "store_info"
        	}
        }
	]
}
```

이렇게 설정하면 여러 index에서의 검색 작업을 한번에 진행할 수 있게 된다.  
